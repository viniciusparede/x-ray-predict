{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# About Notebook\n","\n","This notebook is an example of how to perform classification or regression predictions using images, in this case, x-ray images.\n","\n","The intent of the code below is to encourage developers to build their own Convolutional Neural Network (CNN), test pre-existing models, or even use features extracted from pre-existing models by inputting these into shallow models, such as Decision Trees. The idea here is to explore and potentially leverage the strengths of different model architectures in order to better understand and solve image-based machine learning tasks."]},{"cell_type":"code","execution_count":24,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:13:56.118758Z","iopub.status.busy":"2023-06-27T19:13:56.117508Z","iopub.status.idle":"2023-06-27T19:13:56.127839Z","shell.execute_reply":"2023-06-27T19:13:56.126875Z","shell.execute_reply.started":"2023-06-27T19:13:56.118700Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')\n"]}],"source":["import os\n","\n","import pandas as pd\n","import numpy as np\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import r2_score\n","from sklearn.metrics import mean_absolute_error\n","from sklearn.metrics import mean_squared_error\n","from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n","from sklearn.preprocessing import StandardScaler\n","\n","from PIL import Image\n","\n","import tensorflow as tf\n","from keras.applications import ResNet50\n","from keras.layers import Dense, GlobalAveragePooling2D, Flatten, Conv2D, MaxPooling2D\n","from keras.models import Model, Sequential\n","from keras.losses import BinaryCrossentropy\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras.applications.imagenet_utils import preprocess_input\n","\n","from tqdm import tqdm\n","\n","devices = tf.config.list_physical_devices()\n","for device in devices:\n","    print(device)"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:13:56.130878Z","iopub.status.busy":"2023-06-27T19:13:56.129971Z","iopub.status.idle":"2023-06-27T19:13:56.140910Z","shell.execute_reply":"2023-06-27T19:13:56.139925Z","shell.execute_reply.started":"2023-06-27T19:13:56.130845Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["/home/vinicius/repositories/x-ray-predict/data/kaggle/kaggle\n","/home/vinicius/repositories/x-ray-predict/data/kaggle/kaggle/train\n","/home/vinicius/repositories/x-ray-predict/data/kaggle/kaggle/test \n","\n"]}],"source":["current_dir = os.getcwd()\n","\n","repo_dir = current_dir[:current_dir.find(\"/notebooks\")]\n","\n","data_dir = os.path.join(repo_dir, \"data\")\n","\n","kaggle_dir = os.path.join(data_dir, \"kaggle\")\n","kaggle_dir = os.path.join(kaggle_dir, \"kaggle\")\n","\n","train_dir = os.path.join(kaggle_dir, \"train\")\n","test_dir = os.path.join(kaggle_dir, \"test\")\n","\n","print(kaggle_dir)\n","print(train_dir)\n","print(test_dir, \"\\n\")"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:13:56.143188Z","iopub.status.busy":"2023-06-27T19:13:56.142265Z","iopub.status.idle":"2023-06-27T19:14:06.874808Z","shell.execute_reply":"2023-06-27T19:14:06.873712Z","shell.execute_reply.started":"2023-06-27T19:13:56.143156Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Images :  10702\n","Test Images  :  11747\n"]}],"source":["def get_images_path(directory, extension: str = \".png\") -> list:\n","    \"\"\"\n","    This function returns a list of all file paths that match a given file extension within a directory and its subdirectories.\n","    \n","    Parameters\n","    ----------\n","    directory : str\n","        The path of the directory to be searched. Subdirectories will be included in the search.\n","\n","    extension : str, optional\n","        The file extension of the image files to be searched. Default is \".png\".\n","\n","    Returns\n","    -------\n","    list\n","        A sorted list of file paths of the images found within the directory and its subdirectories that match the specified file extension.\n","\n","    Examples\n","    --------\n","    >>> get_images_path(\"/path/to/your/directory\", \".jpg\")\n","    ['/path/to/your/directory/image1.jpg', '/path/to/your/directory/subdirectory/image2.jpg', ...]\n","    \"\"\"\n","    file_images = [\n","        os.path.join(dirpath, filename)\n","        for dirpath, dirnames, filenames in os.walk(directory)\n","        for filename in filenames\n","        if filename.endswith(extension)\n","    ]\n","\n","    return sorted(file_images)\n","\n","\n","train_images_path = get_images_path(train_dir)\n","test_images_path = get_images_path(test_dir)\n","\n","print(\"Train Images : \", len(train_images_path))\n","print(\"Test Images  : \", len(test_images_path))"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:14:06.880927Z","iopub.status.busy":"2023-06-27T19:14:06.877278Z","iopub.status.idle":"2023-06-27T19:14:06.917798Z","shell.execute_reply":"2023-06-27T19:14:06.916823Z","shell.execute_reply.started":"2023-06-27T19:14:06.880898Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>imageId</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>directory</th>\n","      <th>filename</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>89.0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000000.png</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>72.0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000001.png</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>1</td>\n","      <td>25.0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000002.png</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>68.0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000003.png</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>37.0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000004.png</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   imageId  gender   age                                          directory  \\\n","0        0       0  89.0  /home/vinicius/repositories/x-ray-predict/data...   \n","1        1       0  72.0  /home/vinicius/repositories/x-ray-predict/data...   \n","2        2       1  25.0  /home/vinicius/repositories/x-ray-predict/data...   \n","3        3       1  68.0  /home/vinicius/repositories/x-ray-predict/data...   \n","4        4       0  37.0  /home/vinicius/repositories/x-ray-predict/data...   \n","\n","     filename  \n","0  000000.png  \n","1  000001.png  \n","2  000002.png  \n","3  000003.png  \n","4  000004.png  "]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["train_gender_df = pd.read_csv(os.path.join(data_dir, \"train_gender.csv\"))\n","\n","train_age_df = pd.read_csv(os.path.join(data_dir, \"train_age.csv\"))\n","\n","train_df = pd.merge(left=train_gender_df, right=train_age_df, how=\"inner\", on=\"imageId\")\n","\n","train_df[\"directory\"] = train_images_path\n","\n","train_df[\"filename\"] = train_df[\"directory\"].apply(lambda x: x[-10:])\n","\n","train_df.head()"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:14:06.919882Z","iopub.status.busy":"2023-06-27T19:14:06.919263Z","iopub.status.idle":"2023-06-27T19:14:06.929555Z","shell.execute_reply":"2023-06-27T19:14:06.928488Z","shell.execute_reply.started":"2023-06-27T19:14:06.919848Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Train Dataset:        7491\n","Validation Dataset:   3211\n"]}],"source":["train_df, val_df = train_test_split(\n","    train_df,\n","    test_size=0.3,\n","    random_state=42,\n",")\n","\n","print(\"Train Dataset:       \", train_df.shape[0])\n","print(\"Validation Dataset:  \", val_df.shape[0])"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:14:06.931948Z","iopub.status.busy":"2023-06-27T19:14:06.931247Z","iopub.status.idle":"2023-06-27T19:14:06.940746Z","shell.execute_reply":"2023-06-27T19:14:06.938587Z","shell.execute_reply.started":"2023-06-27T19:14:06.931915Z"},"trusted":true},"outputs":[],"source":["def load_images(df: pd.DataFrame, img_size: tuple) -> np.array:\n","    \"\"\"Loads images and preprocesses them for the model.\"\"\"\n","    image_list = []\n","    for filename in tqdm(df[\"directory\"]):\n","        # Open image and resize\n","        img = Image.open(filename).convert(\"RGB\").resize(img_size)\n","        # Convert image to array and preprocess\n","        img_array = np.array(img)\n","        img_array = preprocess_input(img_array)\n","        image_list.append(img_array)\n","\n","    images = np.array(image_list)\n","    return images"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:14:06.944014Z","iopub.status.busy":"2023-06-27T19:14:06.942670Z","iopub.status.idle":"2023-06-27T19:23:01.829445Z","shell.execute_reply":"2023-06-27T19:23:01.828459Z","shell.execute_reply.started":"2023-06-27T19:14:06.943980Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 7491/7491 [03:52<00:00, 32.26it/s]\n","100%|██████████| 3211/3211 [01:30<00:00, 35.41it/s]\n"]}],"source":["image_size = (64, 64)\n","\n","x_train, x_val = load_images(train_df, image_size), load_images(val_df, image_size)\n","y_gender_train, y_gender_val = train_df[\"gender\"].values, val_df[\"gender\"].values\n","y_age_train, y_age_val = train_df[\"age\"].values, val_df[\"age\"].values"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:33:17.953494Z","iopub.status.busy":"2023-06-27T19:33:17.952465Z","iopub.status.idle":"2023-06-27T19:33:18.349249Z","shell.execute_reply":"2023-06-27T19:33:18.347734Z","shell.execute_reply.started":"2023-06-27T19:33:17.953454Z"},"trusted":true},"outputs":[],"source":["x_train_normalized = x_train / 255\n","x_val_normalized = x_val / 255"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 1. Build own CNN"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:33:26.106394Z","iopub.status.busy":"2023-06-27T19:33:26.106018Z","iopub.status.idle":"2023-06-27T19:33:26.435204Z","shell.execute_reply":"2023-06-27T19:33:26.434541Z","shell.execute_reply.started":"2023-06-27T19:33:26.106361Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"wall_net_classifier\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d (Conv2D)             (None, 62, 62, 32)        896       \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 31, 31, 32)       0         \n"," )                                                               \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 29, 29, 64)        18496     \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 14, 14, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_2 (Conv2D)           (None, 12, 12, 64)        36928     \n","                                                                 \n"," flatten (Flatten)           (None, 9216)              0         \n","                                                                 \n"," dense (Dense)               (None, 64)                589888    \n","                                                                 \n"," dense_1 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 646,273\n","Trainable params: 646,273\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","\n","\n","\n","Model: \"wall_net_regression\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv2d_3 (Conv2D)           (None, 62, 62, 32)        896       \n","                                                                 \n"," max_pooling2d_2 (MaxPooling  (None, 31, 31, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 29, 29, 64)        18496     \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 14, 14, 64)       0         \n"," 2D)                                                             \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 12, 12, 64)        36928     \n","                                                                 \n"," flatten_1 (Flatten)         (None, 9216)              0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                589888    \n","                                                                 \n"," dense_3 (Dense)             (None, 1)                 65        \n","                                                                 \n","=================================================================\n","Total params: 646,273\n","Trainable params: 646,273\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n"]}],"source":["class WallNetClassifier(Sequential):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Adding the first convolutional layer\n","        self.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)))\n","\n","        # Adding a pooling layer to reduce dimensionality\n","        self.add(MaxPooling2D((2, 2)))\n","\n","        # Adding a second convolutional layer\n","        self.add(Conv2D(64, (3, 3), activation=\"relu\"))\n","\n","        # Adding another pooling layer\n","        self.add(MaxPooling2D((2, 2)))\n","\n","        # Adding a third convolutional layer\n","        self.add(Conv2D(64, (3, 3), activation=\"relu\"))\n","\n","        # Adding a Flatten layer to transform the feature matrix into a vector\n","        self.add(Flatten())\n","\n","        # Adding a dense layer (or 'fully connected' layer)\n","        self.add(Dense(64, activation=\"relu\"))\n","\n","        # Adding the output layer\n","        self.add(Dense(1, activation=\"sigmoid\"))\n","\n","        # Compiling the model\n","        self.compile(\n","            optimizer=\"adam\",\n","            loss=BinaryCrossentropy(),\n","            metrics=[\"accuracy\"],\n","        )\n","\n","    def train_gender_model(\n","        self,\n","        X_train: np.array,\n","        y_train: np.array,\n","        X_val: np.array,\n","        y_val: np.array,\n","        epochs: int = 10,\n","        batch_size: int = 32,\n","        verbose: int = 1,\n","    ):\n","        # Fitting the model\n","        self.fit(\n","            X_train,\n","            y_train,\n","            validation_data=(X_val, y_val),\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            verbose=verbose,\n","        )\n","        \n","    def predict_gender(self, X: np.array):\n","        \n","        predictions = self.predict(X)\n","\n","        predicted_classes = (predictions > 0.5).astype(int)\n","\n","        return predicted_classes\n","    \n","\n","\n","\n","class WallNetRegression(Sequential):\n","    def __init__(self):\n","        super().__init__()\n","\n","        # Adding the first convolutional layer\n","        self.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(64, 64, 3)))\n","\n","        # Adding a pooling layer to reduce dimensionality\n","        self.add(MaxPooling2D((2, 2)))\n","\n","        # Adding a second convolutional layer\n","        self.add(Conv2D(64, (3, 3), activation=\"relu\"))\n","\n","        # Adding another pooling layer\n","        self.add(MaxPooling2D((2, 2)))\n","\n","        # Adding a third convolutional layer\n","        self.add(Conv2D(64, (3, 3), activation=\"relu\"))\n","\n","        # Adding a Flatten layer to transform the feature matrix into a vector\n","        self.add(Flatten())\n","\n","        # Adding a dense layer (or 'fully connected' layer)\n","        self.add(Dense(64, activation=\"relu\"))\n","\n","        # Adding the output layer\n","        # Linear activation for regression\n","        self.add(Dense(1, activation=\"linear\"))  \n","\n","        # Compiling the model\n","        self.compile(\n","            optimizer=\"adam\",\n","            loss=\"mse\",\n","            metrics=[\"mae\"]\n","        )\n","        \n","    def train_age(\n","        self,\n","        X_train: np.array,\n","        y_train: np.array,\n","        X_val: np.array,\n","        y_val: np.array,\n","        epochs: int = 10,\n","        batch_size: int = 32,\n","        verbose: int = 1,\n","    ):\n","        # Fitting the model\n","        self.fit(\n","            X_train,\n","            y_train,\n","            validation_data=(X_val, y_val),\n","            epochs=epochs,\n","            batch_size=batch_size,\n","            verbose=verbose,\n","        )\n","\n","    def predict_age(self, X: np.array):\n","        # Predicting age\n","        return self.predict(X)\n","    \n","wallnet_classifier = WallNetClassifier()\n","print(wallnet_classifier.summary())\n","\n","print(\"\\n\\n\\n\")\n","    \n","wallnet_regression = WallNetRegression()\n","print(wallnet_regression.summary())"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Gender"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:33:31.032895Z","iopub.status.busy":"2023-06-27T19:33:31.032502Z","iopub.status.idle":"2023-06-27T19:33:50.178217Z","shell.execute_reply":"2023-06-27T19:33:50.177266Z","shell.execute_reply.started":"2023-06-27T19:33:31.032863Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["2023-06-27 19:50:08.614424: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 368197632 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["234/235 [============================>.] - ETA: 0s - loss: 0.4118 - accuracy: 0.8180"]},{"name":"stderr","output_type":"stream","text":["2023-06-27 19:50:27.518857: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157827072 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["235/235 [==============================] - 21s 85ms/step - loss: 0.4116 - accuracy: 0.8180 - val_loss: 0.3442 - val_accuracy: 0.8564\n","Epoch 2/10\n","235/235 [==============================] - 22s 92ms/step - loss: 0.2576 - accuracy: 0.8980 - val_loss: 0.2463 - val_accuracy: 0.8950\n","Epoch 3/10\n","235/235 [==============================] - 19s 80ms/step - loss: 0.1905 - accuracy: 0.9286 - val_loss: 0.1946 - val_accuracy: 0.9290\n","Epoch 4/10\n","235/235 [==============================] - 18s 79ms/step - loss: 0.1589 - accuracy: 0.9389 - val_loss: 0.2023 - val_accuracy: 0.9321\n","Epoch 5/10\n","235/235 [==============================] - 20s 83ms/step - loss: 0.1321 - accuracy: 0.9519 - val_loss: 0.1780 - val_accuracy: 0.9383\n","Epoch 6/10\n","235/235 [==============================] - 20s 86ms/step - loss: 0.1119 - accuracy: 0.9602 - val_loss: 0.1789 - val_accuracy: 0.9377\n","Epoch 7/10\n","235/235 [==============================] - 19s 81ms/step - loss: 0.0930 - accuracy: 0.9668 - val_loss: 0.2058 - val_accuracy: 0.9402\n","Epoch 8/10\n","235/235 [==============================] - 19s 81ms/step - loss: 0.0747 - accuracy: 0.9760 - val_loss: 0.1941 - val_accuracy: 0.9396\n","Epoch 9/10\n","235/235 [==============================] - 19s 81ms/step - loss: 0.0607 - accuracy: 0.9805 - val_loss: 0.2199 - val_accuracy: 0.9408\n","Epoch 10/10\n","235/235 [==============================] - 19s 80ms/step - loss: 0.0602 - accuracy: 0.9784 - val_loss: 0.2084 - val_accuracy: 0.9449\n"]}],"source":["# fit model gender\n","wallnet_classifier.train_gender_model(X_train=x_train_normalized, y_train=y_gender_train, X_val=x_val_normalized, y_val=y_gender_val)"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:29:04.315042Z","iopub.status.busy":"2023-06-27T18:29:04.314683Z","iopub.status.idle":"2023-06-27T18:29:05.095046Z","shell.execute_reply":"2023-06-27T18:29:05.093232Z","shell.execute_reply.started":"2023-06-27T18:29:04.315006Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["  5/101 [>.............................] - ETA: 1s "]},{"name":"stderr","output_type":"stream","text":["2023-06-27 19:53:24.168193: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157827072 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["101/101 [==============================] - 2s 15ms/step\n","\n","\n","[[1790   96]\n"," [  81 1244]] \n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.95      0.95      1886\n","           1       0.93      0.94      0.93      1325\n","\n","    accuracy                           0.94      3211\n","   macro avg       0.94      0.94      0.94      3211\n","weighted avg       0.95      0.94      0.94      3211\n","\n"]}],"source":["# predict with validation images\n","y_pred = wallnet_classifier.predict_gender(x_val_normalized)\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_gender_val, y_pred)\n","\n","# Report Confusion Matrix\n","report = classification_report(y_gender_val, y_pred)\n","\n","print(\"\\n\")\n","print(cm, \"\\n\\n\")\n","print(report)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Age"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:35:14.101935Z","iopub.status.busy":"2023-06-27T19:35:14.101432Z","iopub.status.idle":"2023-06-27T19:35:36.207106Z","shell.execute_reply":"2023-06-27T19:35:36.206047Z","shell.execute_reply.started":"2023-06-27T19:35:14.101892Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]},{"name":"stderr","output_type":"stream","text":["2023-06-27 19:53:25.999758: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 368197632 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["234/235 [============================>.] - ETA: 0s - loss: 435.9152 - mae: 16.8951"]},{"name":"stderr","output_type":"stream","text":["2023-06-27 19:53:44.198716: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 157827072 exceeds 10% of free system memory.\n"]},{"name":"stdout","output_type":"stream","text":["235/235 [==============================] - 20s 81ms/step - loss: 435.8469 - mae: 16.8944 - val_loss: 333.6973 - val_mae: 15.1526\n","Epoch 2/10\n","235/235 [==============================] - 20s 86ms/step - loss: 253.0375 - mae: 13.0141 - val_loss: 256.9884 - val_mae: 12.9510\n","Epoch 3/10\n","235/235 [==============================] - 20s 86ms/step - loss: 207.6963 - mae: 11.6391 - val_loss: 189.7469 - val_mae: 11.1897\n","Epoch 4/10\n","235/235 [==============================] - 18s 78ms/step - loss: 176.5767 - mae: 10.7421 - val_loss: 159.9057 - val_mae: 10.2216\n","Epoch 5/10\n","235/235 [==============================] - 19s 80ms/step - loss: 157.9096 - mae: 10.1033 - val_loss: 163.8023 - val_mae: 10.3232\n","Epoch 6/10\n","235/235 [==============================] - 21s 88ms/step - loss: 145.2508 - mae: 9.6478 - val_loss: 150.8340 - val_mae: 9.7713\n","Epoch 7/10\n","235/235 [==============================] - 20s 83ms/step - loss: 133.5338 - mae: 9.2056 - val_loss: 145.1154 - val_mae: 9.7205\n","Epoch 8/10\n","235/235 [==============================] - 20s 83ms/step - loss: 129.9196 - mae: 9.0493 - val_loss: 133.9039 - val_mae: 9.2735\n","Epoch 9/10\n","235/235 [==============================] - 20s 84ms/step - loss: 121.4580 - mae: 8.7156 - val_loss: 141.7081 - val_mae: 9.4320\n","Epoch 10/10\n","235/235 [==============================] - 18s 76ms/step - loss: 115.6255 - mae: 8.4868 - val_loss: 143.3225 - val_mae: 9.4721\n"]}],"source":["# fit model age\n","wallnet_regression.train_age(X_train=x_train_normalized, y_train=y_age_train, X_val=x_val_normalized, y_val=y_age_val, epochs=10, batch_size=32)"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T19:35:37.002388Z","iopub.status.busy":"2023-06-27T19:35:37.001910Z","iopub.status.idle":"2023-06-27T19:35:37.783264Z","shell.execute_reply":"2023-06-27T19:35:37.782219Z","shell.execute_reply.started":"2023-06-27T19:35:37.002346Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["101/101 [==============================] - 2s 16ms/step\n","\n","============ Regression model report ============\n","Root Mean Squared Error (RMSE): 11.97\n","Mean Absolute Error (MAE): 9.47\n","R^2 Score: 0.6\n","=================================================\n"]}],"source":["# predict age with validation images\n","y_pred = wallnet_regression.predict_age(x_val_normalized)\n","\n","rmse = mean_squared_error(y_age_val, y_pred, squared=False).round(2)\n","mae = mean_absolute_error(y_age_val, y_pred).round(2)\n","r2 = r2_score(y_age_val, y_pred).round(2)\n","\n","print(\"\\n============ Regression model report ============\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","print(f\"Mean Absolute Error (MAE): {mae}\")\n","print(f\"R^2 Score: {r2}\")\n","print(\"=================================================\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 2. ResNet50 - FineTunning\n","Using ResNet50, simply change the specific layer to suit the classification or regression problem."]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:29:27.831949Z","iopub.status.busy":"2023-06-27T18:29:27.831566Z","iopub.status.idle":"2023-06-27T18:29:33.106854Z","shell.execute_reply":"2023-06-27T18:29:33.105765Z","shell.execute_reply.started":"2023-06-27T18:29:27.831915Z"},"trusted":true},"outputs":[],"source":["class ResNet50Tunning:\n","    def __init__(self):\n","        self.base_model = ResNet50(\n","            weights=\"imagenet\", include_top=False, input_shape=(64, 64, 3)\n","        )\n","        self.gender_model = None\n","        self.age_model = None\n","\n","    def train_gender_model(\n","        self, X_train: np.array, y_train: np.array, X_val: np.array, y_val: np.array\n","    ):\n","        # Freeze the base layers\n","        for layer in self.base_model.layers:\n","            layer.trainable = False\n","\n","        # Add custom layers for gender classification\n","        pooled_output = GlobalAveragePooling2D()(self.base_model.output)\n","        gender_output = Dense(1, activation=\"sigmoid\")(pooled_output)\n","        self.gender_model = Model(inputs=self.base_model.input, outputs=gender_output)\n","\n","        # Compile and train the gender classification model\n","        self.gender_model.compile(\n","            optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"]\n","        )\n","\n","        self.gender_model.fit(\n","            X_train,\n","            y_train,\n","            validation_data=(X_val, y_val),\n","            epochs=10,\n","            batch_size=32,\n","        )\n","\n","        return self.gender_model\n","\n","    def train_age_model(\n","        self, X_train: np.array, y_train: np.array, X_val: np.array, y_val: np.array\n","    ):\n","        # Freeze the base layers\n","        for layer in self.base_model.layers:\n","            layer.trainable = False\n","\n","        # Add custom layers for age regression\n","        pooled_output = GlobalAveragePooling2D()(self.base_model.output)\n","        age_output = Dense(1, activation=\"linear\")(pooled_output)\n","        self.age_model = Model(inputs=self.base_model.input, outputs=age_output)\n","\n","        # Compile and train the age regression model\n","        self.age_model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"mae\"])\n","\n","        self.age_model.fit(\n","            X_train,\n","            y_train,\n","            validation_data=(X_val, y_val),\n","            epochs=10,\n","            batch_size=32,\n","        )\n","\n","        return self.age_model\n","    \n","fine_tunning = ResNet50Tunning()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Gender"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:29:33.108575Z","iopub.status.busy":"2023-06-27T18:29:33.108216Z","iopub.status.idle":"2023-06-27T18:30:58.919971Z","shell.execute_reply":"2023-06-27T18:30:58.918832Z","shell.execute_reply.started":"2023-06-27T18:29:33.108541Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","235/235 [==============================] - 55s 225ms/step - loss: 0.6612 - accuracy: 0.5985 - val_loss: 0.6439 - val_accuracy: 0.6191\n","Epoch 2/10\n","235/235 [==============================] - 52s 220ms/step - loss: 0.6307 - accuracy: 0.6464 - val_loss: 0.6270 - val_accuracy: 0.6609\n","Epoch 3/10\n","235/235 [==============================] - 50s 211ms/step - loss: 0.6130 - accuracy: 0.6695 - val_loss: 0.6146 - val_accuracy: 0.6702\n","Epoch 4/10\n","235/235 [==============================] - 50s 215ms/step - loss: 0.6007 - accuracy: 0.6852 - val_loss: 0.6063 - val_accuracy: 0.6802\n","Epoch 5/10\n","235/235 [==============================] - 54s 229ms/step - loss: 0.5911 - accuracy: 0.6918 - val_loss: 0.6020 - val_accuracy: 0.6724\n","Epoch 6/10\n","235/235 [==============================] - 55s 235ms/step - loss: 0.5840 - accuracy: 0.6987 - val_loss: 0.5940 - val_accuracy: 0.6836\n","Epoch 7/10\n","235/235 [==============================] - 69s 295ms/step - loss: 0.5791 - accuracy: 0.7052 - val_loss: 0.5893 - val_accuracy: 0.6979\n","Epoch 8/10\n","235/235 [==============================] - 61s 260ms/step - loss: 0.5725 - accuracy: 0.7060 - val_loss: 0.5850 - val_accuracy: 0.6985\n","Epoch 9/10\n","235/235 [==============================] - 60s 257ms/step - loss: 0.5685 - accuracy: 0.7141 - val_loss: 0.5932 - val_accuracy: 0.6883\n","Epoch 10/10\n","235/235 [==============================] - 54s 231ms/step - loss: 0.5657 - accuracy: 0.7147 - val_loss: 0.5779 - val_accuracy: 0.7076\n"]}],"source":["gender_model = fine_tunning.train_gender_model(X_train=x_train_normalized, y_train=y_gender_train, X_val=x_val_normalized, y_val=y_gender_val)"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:30:58.922844Z","iopub.status.busy":"2023-06-27T18:30:58.922437Z","iopub.status.idle":"2023-06-27T18:31:01.312155Z","shell.execute_reply":"2023-06-27T18:31:01.311179Z","shell.execute_reply.started":"2023-06-27T18:30:58.922809Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["101/101 [==============================] - 17s 164ms/step\n","\n","\n","[[1608  278]\n"," [ 661  664]] \n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.71      0.85      0.77      1886\n","           1       0.70      0.50      0.59      1325\n","\n","    accuracy                           0.71      3211\n","   macro avg       0.71      0.68      0.68      3211\n","weighted avg       0.71      0.71      0.70      3211\n","\n"]}],"source":["# predict with validation images\n","predictions = gender_model.predict(x_val_normalized)\n","\n","y_pred = (predictions > 0.5).astype(int)\n","\n","# Confusion Matrix\n","cm = confusion_matrix(y_gender_val, y_pred)\n","\n","# Report Confusion Matrix\n","report = classification_report(y_gender_val, y_pred)\n","\n","print(\"\\n\")\n","print(cm, \"\\n\\n\")\n","print(report)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Age"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:31:01.314205Z","iopub.status.busy":"2023-06-27T18:31:01.313614Z","iopub.status.idle":"2023-06-27T18:32:27.168487Z","shell.execute_reply":"2023-06-27T18:32:27.167435Z","shell.execute_reply.started":"2023-06-27T18:31:01.314161Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","235/235 [==============================] - 53s 216ms/step - loss: 2165.5774 - mae: 42.2636 - val_loss: 1475.3674 - val_mae: 33.3562\n","Epoch 2/10\n","235/235 [==============================] - 49s 208ms/step - loss: 1085.6484 - mae: 27.2774 - val_loss: 758.8674 - val_mae: 21.8663\n","Epoch 3/10\n","235/235 [==============================] - 52s 223ms/step - loss: 598.9350 - mae: 19.5086 - val_loss: 473.7922 - val_mae: 17.6046\n","Epoch 4/10\n","235/235 [==============================] - 56s 240ms/step - loss: 421.3228 - mae: 17.1070 - val_loss: 386.4418 - val_mae: 16.7223\n","Epoch 5/10\n","235/235 [==============================] - 54s 231ms/step - loss: 371.4050 - mae: 16.6664 - val_loss: 366.5685 - val_mae: 16.6462\n","Epoch 6/10\n","235/235 [==============================] - 55s 233ms/step - loss: 361.3814 - mae: 16.6525 - val_loss: 363.7717 - val_mae: 16.6937\n","Epoch 7/10\n","235/235 [==============================] - 58s 247ms/step - loss: 359.5862 - mae: 16.6747 - val_loss: 363.4723 - val_mae: 16.7289\n","Epoch 8/10\n","235/235 [==============================] - 54s 229ms/step - loss: 359.1989 - mae: 16.6803 - val_loss: 363.3584 - val_mae: 16.7368\n","Epoch 9/10\n","235/235 [==============================] - 53s 226ms/step - loss: 359.0328 - mae: 16.6967 - val_loss: 362.9805 - val_mae: 16.7178\n","Epoch 10/10\n","235/235 [==============================] - 55s 234ms/step - loss: 358.8842 - mae: 16.6832 - val_loss: 362.6807 - val_mae: 16.7088\n"]}],"source":["age_model = fine_tunning.train_age_model(X_train=x_train_normalized, y_train=y_age_train, X_val=x_val_normalized, y_val=y_age_val)"]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:27.170858Z","iopub.status.busy":"2023-06-27T18:32:27.170299Z","iopub.status.idle":"2023-06-27T18:32:29.627464Z","shell.execute_reply":"2023-06-27T18:32:29.626528Z","shell.execute_reply.started":"2023-06-27T18:32:27.170829Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["101/101 [==============================] - 17s 162ms/step\n","\n","============ Regression model report ============\n","Root Mean Squared Error (RMSE): 19.04\n","Mean Absolute Error (MAE): 16.71\n","R^2 Score: -0.0\n","=================================================\n"]}],"source":["# predict age with validation images\n","y_pred = age_model.predict(x_val_normalized)\n","\n","rmse = mean_squared_error(y_age_val, y_pred, squared=False).round(2)\n","mae = mean_absolute_error(y_age_val, y_pred).round(2)\n","r2 = r2_score(y_age_val, y_pred).round(2)\n","\n","print(\"\\n============ Regression model report ============\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","print(f\"Mean Absolute Error (MAE): {mae}\")\n","print(f\"R^2 Score: {r2}\")\n","print(\"=================================================\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 3. ResNet50 - Transfer Learning\n","Extract features to DecisionTree Model"]},{"cell_type":"code","execution_count":42,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:29.629293Z","iopub.status.busy":"2023-06-27T18:32:29.628950Z","iopub.status.idle":"2023-06-27T18:32:37.082994Z","shell.execute_reply":"2023-06-27T18:32:37.081967Z","shell.execute_reply.started":"2023-06-27T18:32:29.629260Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["235/235 [==============================] - 39s 162ms/step\n","101/101 [==============================] - 15s 145ms/step\n"]}],"source":["base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(64, 64, 3))\n","model = Model(inputs=base_model.input, outputs=base_model.output)\n","\n","features_train = model.predict(x_train_normalized)\n","features_val = model.predict(x_val_normalized)\n","\n","features_train = features_train.reshape(features_train.shape[0], -1)\n","features_val = features_val.reshape(features_val.shape[0], -1)"]},{"cell_type":"code","execution_count":43,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:37.086960Z","iopub.status.busy":"2023-06-27T18:32:37.086622Z","iopub.status.idle":"2023-06-27T18:32:38.045066Z","shell.execute_reply":"2023-06-27T18:32:38.044035Z","shell.execute_reply.started":"2023-06-27T18:32:37.086933Z"},"trusted":true},"outputs":[],"source":["scaler = StandardScaler()\n","\n","features_train_standardize = scaler.fit_transform(features_train)\n","features_val_standardize = scaler.transform(features_val)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Gender"]},{"cell_type":"code","execution_count":44,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:38.046907Z","iopub.status.busy":"2023-06-27T18:32:38.046514Z","iopub.status.idle":"2023-06-27T18:32:40.645458Z","shell.execute_reply":"2023-06-27T18:32:40.644363Z","shell.execute_reply.started":"2023-06-27T18:32:38.046872Z"},"trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"],"text/plain":["DecisionTreeClassifier()"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["clf = DecisionTreeClassifier()\n","\n","clf.fit(features_train_standardize, y_gender_train)"]},{"cell_type":"code","execution_count":45,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:40.647542Z","iopub.status.busy":"2023-06-27T18:32:40.646947Z","iopub.status.idle":"2023-06-27T18:32:40.664981Z","shell.execute_reply":"2023-06-27T18:32:40.664121Z","shell.execute_reply.started":"2023-06-27T18:32:40.647505Z"},"trusted":true},"outputs":[],"source":["y_pred = clf.predict(features_val_standardize)"]},{"cell_type":"code","execution_count":46,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:40.666720Z","iopub.status.busy":"2023-06-27T18:32:40.666295Z","iopub.status.idle":"2023-06-27T18:32:40.687207Z","shell.execute_reply":"2023-06-27T18:32:40.685926Z","shell.execute_reply.started":"2023-06-27T18:32:40.666688Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","\n","[[1384  502]\n"," [ 460  865]] \n","\n","\n","              precision    recall  f1-score   support\n","\n","           0       0.75      0.73      0.74      1886\n","           1       0.63      0.65      0.64      1325\n","\n","    accuracy                           0.70      3211\n","   macro avg       0.69      0.69      0.69      3211\n","weighted avg       0.70      0.70      0.70      3211\n","\n"]}],"source":["# Confusion Matrix\n","cm = confusion_matrix(y_gender_val, y_pred)\n","\n","# Report Confusion Matrix\n","report = classification_report(y_gender_val, y_pred)\n","\n","print(\"\\n\")\n","print(cm, \"\\n\\n\")\n","print(report)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Age"]},{"cell_type":"code","execution_count":47,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:40.689158Z","iopub.status.busy":"2023-06-27T18:32:40.688815Z","iopub.status.idle":"2023-06-27T18:32:43.889659Z","shell.execute_reply":"2023-06-27T18:32:43.888713Z","shell.execute_reply.started":"2023-06-27T18:32:40.689127Z"},"trusted":true},"outputs":[{"data":{"text/html":["<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeRegressor()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeRegressor</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeRegressor()</pre></div></div></div></div></div>"],"text/plain":["DecisionTreeRegressor()"]},"execution_count":47,"metadata":{},"output_type":"execute_result"}],"source":["clf = DecisionTreeRegressor()\n","\n","clf.fit(features_train_standardize, y_age_train)"]},{"cell_type":"code","execution_count":48,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:43.891469Z","iopub.status.busy":"2023-06-27T18:32:43.891107Z","iopub.status.idle":"2023-06-27T18:32:43.909872Z","shell.execute_reply":"2023-06-27T18:32:43.909021Z","shell.execute_reply.started":"2023-06-27T18:32:43.891436Z"},"trusted":true},"outputs":[],"source":["y_pred = clf.predict(features_val_standardize)"]},{"cell_type":"code","execution_count":49,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:43.911663Z","iopub.status.busy":"2023-06-27T18:32:43.911299Z","iopub.status.idle":"2023-06-27T18:32:43.920754Z","shell.execute_reply":"2023-06-27T18:32:43.919760Z","shell.execute_reply.started":"2023-06-27T18:32:43.911616Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","============ Regression model report ============\n","Root Mean Squared Error (RMSE): 23.29\n","Mean Absolute Error (MAE): 18.2\n","R^2 Score: -0.5\n","=================================================\n"]}],"source":["rmse = mean_squared_error(y_age_val, y_pred, squared=False).round(2)\n","mae = mean_absolute_error(y_age_val, y_pred).round(2)\n","r2 = r2_score(y_age_val, y_pred).round(2)\n","\n","print(\"\\n============ Regression model report ============\")\n","print(f\"Root Mean Squared Error (RMSE): {rmse}\")\n","print(f\"Mean Absolute Error (MAE): {mae}\")\n","print(f\"R^2 Score: {r2}\")\n","print(\"=================================================\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["This is the worst possible outcome. It is evident that extracting features through ResNet50 and feeding them to a Decision Tree Regressor model isn't the best scenario. The model fails to generalize for validation data, resulting in a negative R-squared value and high Mean Absolute Error (MAE). This suggests that the model performs worse than a hypothetical model that always predicts the mean of the target variable. The negative R-squared indicates that the model cannot account for the variability in the response variable around its mean, and the high MAE shows that our predictions are, on average, far from the actual values."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# 4. Submit Result"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The technique that yielded the best result was the creation of our own Convolutional Neural Network, 'WallNet'.\n","\n","It demonstrated an accuracy of 94% for gender classification and had the best evaluation metrics for age regression of the patients. Given this scenario, we can submit our final result.\n","\n","* The evaluation metric for gender competition is the area under the receiver operating characteristic curve (AUC)\n","* The evaluation metric for age competition is the mean absolute error (MAE)"]},{"cell_type":"code","execution_count":51,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:43.922863Z","iopub.status.busy":"2023-06-27T18:32:43.922256Z","iopub.status.idle":"2023-06-27T18:32:43.983499Z","shell.execute_reply":"2023-06-27T18:32:43.982555Z","shell.execute_reply.started":"2023-06-27T18:32:43.922810Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>imageId</th>\n","      <th>gender</th>\n","      <th>age</th>\n","      <th>directory</th>\n","      <th>filename</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.5</td>\n","      <td>0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000000.png</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.5</td>\n","      <td>0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000001.png</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000002.png</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.5</td>\n","      <td>0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000003.png</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.5</td>\n","      <td>0</td>\n","      <td>/home/vinicius/repositories/x-ray-predict/data...</td>\n","      <td>000004.png</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   imageId  gender  age                                          directory  \\\n","0        0     0.5    0  /home/vinicius/repositories/x-ray-predict/data...   \n","1        1     0.5    0  /home/vinicius/repositories/x-ray-predict/data...   \n","2        2     0.5    0  /home/vinicius/repositories/x-ray-predict/data...   \n","3        3     0.5    0  /home/vinicius/repositories/x-ray-predict/data...   \n","4        4     0.5    0  /home/vinicius/repositories/x-ray-predict/data...   \n","\n","     filename  \n","0  000000.png  \n","1  000001.png  \n","2  000002.png  \n","3  000003.png  \n","4  000004.png  "]},"execution_count":51,"metadata":{},"output_type":"execute_result"}],"source":["test_gender_df = pd.read_csv(os.path.join(data_dir, \"sample_submission_gender.csv\"))\n","test_age_df = pd.read_csv(os.path.join(data_dir, \"sample_submission_age.csv\"))\n","test_df = pd.merge(left=test_gender_df, right=test_age_df, how=\"inner\", on=\"imageId\")\n","\n","test_df[\"directory\"] = test_images_path\n","test_df[\"filename\"] = test_df[\"directory\"].apply(lambda x: x[-10:])\n","\n","test_df.head()"]},{"cell_type":"code","execution_count":52,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:32:43.985166Z","iopub.status.busy":"2023-06-27T18:32:43.984839Z","iopub.status.idle":"2023-06-27T18:43:17.020840Z","shell.execute_reply":"2023-06-27T18:43:17.018752Z","shell.execute_reply.started":"2023-06-27T18:32:43.985143Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 11747/11747 [04:37<00:00, 42.31it/s]\n"]}],"source":["image_size = (64, 64)\n","x_test = load_images(test_df, image_size)\n","x_test_normalized = x_test / 255"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Gender"]},{"cell_type":"code","execution_count":53,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:48:26.297935Z","iopub.status.busy":"2023-06-27T18:48:26.296752Z","iopub.status.idle":"2023-06-27T18:48:30.250929Z","shell.execute_reply":"2023-06-27T18:48:30.249972Z","shell.execute_reply.started":"2023-06-27T18:48:26.297891Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["368/368 [==============================] - 7s 18ms/step\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>imageId</th>\n","      <th>gender</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.895959</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.005626</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.999929</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.021404</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.997383</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   imageId    gender\n","0        0  0.895959\n","1        1  0.005626\n","2        2  0.999929\n","3        3  0.021404\n","4        4  0.997383"]},"execution_count":53,"metadata":{},"output_type":"execute_result"}],"source":["gender_submission_df = test_df.loc[:, [\"imageId\", \"gender\"]].copy()\n","\n","gender_submission_df[\"gender\"] = wallnet_classifier.predict(x_test_normalized)\n","\n","gender_submission_df.to_csv(\"sample_submission_gender.csv\", index=False, sep=\",\", decimal=\".\")\n","\n","gender_submission_df.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Age"]},{"cell_type":"code","execution_count":54,"metadata":{"execution":{"iopub.execute_input":"2023-06-27T18:44:53.763531Z","iopub.status.busy":"2023-06-27T18:44:53.763148Z","iopub.status.idle":"2023-06-27T18:44:56.258046Z","shell.execute_reply":"2023-06-27T18:44:56.257042Z","shell.execute_reply.started":"2023-06-27T18:44:53.763498Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["368/368 [==============================] - 6s 17ms/step\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>imageId</th>\n","      <th>age</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>49</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>37</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>68</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>51</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   imageId  age\n","0        0   49\n","1        1   37\n","2        2   41\n","3        3   68\n","4        4   51"]},"execution_count":54,"metadata":{},"output_type":"execute_result"}],"source":["age_submission_df = test_df.loc[:, [\"imageId\", \"age\"]].copy()\n","\n","age_submission_df[\"age\"] = wallnet_regression.predict(x_test_normalized).astype(int)\n","\n","age_submission_df.to_csv(\"sample_submission_age.csv\", index=False, sep=\",\", decimal=\".\")\n","\n","age_submission_df.head()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.2"}},"nbformat":4,"nbformat_minor":4}
